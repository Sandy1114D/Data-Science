{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What\n",
        "are its main components?\n",
        "2. What are the different types of activation functions popularly used? Explain each of them.\n",
        "3.\n",
        "1. Explain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a\n",
        "simple perceptron?\n",
        "2. Use a simple perceptron with weights w 0 , w 1 , and w 2  as −1, 2, and 1, respectively, to classify\n",
        "data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n",
        "2. Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR\n",
        "problem.\n",
        "3. What is artificial neural network (ANN)? Explain some of the salient highlights in the\n",
        "different architectural options for ANN.\n",
        "4. Explain the learning process of an ANN. Explain, with example, the challenge in assigning\n",
        "synaptic weights for the interconnection between neurons? How can this challenge be\n",
        "addressed?\n",
        "5. Explain, in details, the backpropagation algorithm. What are the limitations of this\n",
        "algorithm?\n",
        "6. Describe, in details, the process of adjusting the interconnection weights in a multi-layer\n",
        "neural network.\n",
        "7. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is\n",
        "required?\n",
        "8. Write short notes on:\n",
        "\n",
        "1. Artificial neuron\n",
        "2. Multi-layer perceptron\n",
        "3. Deep learning\n",
        "4. Learning rate\n",
        "2. Write the difference between:-\n",
        "\n",
        "1. Activation function vs threshold function\n",
        "2. Step function vs sigmoid function\n",
        "3. Single layer vs multi-layer perceptron"
      ],
      "metadata": {
        "id": "P-AXtlLk4zD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTIONS ARE AS FOLLOE**\n"
      ],
      "metadata": {
        "id": "iGXtzIY_5NAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. An artificial neuron is a basic computational unit in artificial neural networks. It consists of an input layer, a weighted sum function, an activation function, and an output layer. The input layer receives input signals and each input signal is multiplied by a weight value. These weighted inputs are then summed together to produce a weighted sum. The activation function applies a non-linear transformation to the weighted sum, and the output layer produces the final output. The structure of an artificial neuron is similar to a biological neuron in that it receives input signals, processes them, and produces an output signal.\n",
        "\n",
        "2. Different types of activation functions popularly used in artificial neural networks are:\n",
        "\n",
        "- Sigmoid function: The sigmoid function maps any input value to a value between 0 and 1. It is used for binary classification problems.\n",
        "\n",
        "- ReLU (Rectified Linear Unit) function: The ReLU function maps any input value less than zero to zero and any input value greater than or equal to zero to itself. It is used for multi-class classification problems.\n",
        "\n",
        "- Tanh (Hyperbolic Tangent) function: The tanh function maps any input value to a value between -1 and 1. It is used for binary classification problems.\n",
        "\n",
        "3.\n",
        "1. Rosenblatt’s perceptron model is a linear binary classification model that uses a weighted sum of input features to make predictions. It takes the weighted sum of the input features and applies an activation function to produce the output. A set of data can be classified using a simple perceptron by training it on a labeled dataset, adjusting the weights during training, and evaluating the model's performance on a separate test dataset.\n",
        "\n",
        "2. Using the weights w0=-1, w1=2, and w2=1, the perceptron can classify the data points as follows: (3, 4) = 1, (5, 2) = 1, (1, -3) = -1, (-8, -3) = -1, (-3, 0) = -1.\n",
        "\n",
        "4. A multi-layer perceptron (MLP) is an artificial neural network with one or more hidden layers between the input and output layers. It can solve the XOR problem by using non-linear activation functions in the hidden layer(s) that allow it to learn complex non-linear relationships between the input and output.\n",
        "\n",
        "5. Artificial neural network (ANN) is a computational system inspired by the structure and function of biological neural networks. Some salient highlights of the different architectural options for ANN are: \n",
        "- Feedforward neural network\n",
        "- Recurrent neural network\n",
        "- Convolutional neural network\n",
        "- Deep neural network\n",
        "\n",
        "6. The learning process of an ANN involves adjusting the synaptic weights of the connections between neurons to minimize the difference between the actual output and the desired output. The challenge in assigning synaptic weights for the interconnection between neurons is that there can be an extremely large number of possible weight combinations, and finding the optimal set of weights can be a complex optimization problem. This challenge can be addressed by using optimization algorithms such as gradient descent or stochastic gradient descent.\n",
        "\n",
        "7. Backpropagation algorithm is a supervised learning algorithm used to train artificial neural networks. It involves computing the error between the actual output and the desired output, propagating the error back through the network to adjust the synaptic weights, and repeating the process until the error is minimized. The limitations of this algorithm include the potential for the network to get stuck in a local minimum and the difficulty of training very deep networks.\n",
        "\n",
        "8.\n",
        "1. Artificial neuron: A basic computational unit in artificial neural networks that processes input signals and produces an output signal.\n",
        "\n",
        "2. Multi-layer perceptron: An artificial neural network with one or more hidden layers between the input and output layers.\n",
        "\n",
        "3. Deep learning: A subfield of"
      ],
      "metadata": {
        "id": "lkoIJryR5SWv"
      }
    }
  ]
}