{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What does a SavedModel contain? How do you inspect its content?\n",
        "2. When should you use TF Serving? What are its main features? What are some tools you can\n",
        "use to deploy it?\n",
        "3. How do you deploy a model across multiple TF Serving instances?\n",
        "4. When should you use the gRPC API rather than the REST API to query a model served by TF\n",
        "Serving?\n",
        "5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or\n",
        "embedded device?\n",
        "6. What is quantization-aware training, and why would you need it?\n",
        "7. What are model parallelism and data parallelism? Why is the latter\n",
        "generally recommended?\n",
        "8. When training a model across multiple servers, what distribution strategies can you use?\n",
        "How do you choose which one to use?"
      ],
      "metadata": {
        "id": "DDAZzXy7-PEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTIONS ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "90KUgx4l-Spe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A SavedModel contains the TensorFlow graph and its associated variables, assets, and metadata. You can inspect its content using the SavedModel CLI tool or by loading it into a new TensorFlow session.\n",
        "2. You should use TF Serving when you want to serve your models in a scalable, production-ready manner. Its main features include support for multiple models and versions, model management, and low latency serving. Some tools you can use to deploy it include Docker, Kubernetes, and TensorFlow Extended (TFX).\n",
        "3. You can deploy a model across multiple TF Serving instances using a load balancer that distributes requests among them.\n",
        "4. You should use the gRPC API rather than the REST API to query a model served by TF Serving when you require low latency and high throughput, as the gRPC API is optimized for such scenarios.\n",
        "5. TFLite reduces a model's size to make it run on a mobile or embedded device by using techniques such as quantization, pruning, and weight sharing. It can also convert a model to a more efficient format optimized for mobile and embedded devices.\n",
        "6. Quantization-aware training is a training technique where weights and activations are quantized during training to make the resulting model more efficient for deployment on mobile or embedded devices. It is necessary because these devices typically have limited computing resources.\n",
        "7. Model parallelism and data parallelism are two techniques used to distribute the training of a deep learning model across multiple devices or machines. Data parallelism is generally recommended because it is simpler to implement and provides better scalability.\n",
        "8. When training a model across multiple servers, you can use distribution strategies such as MirroredStrategy, MultiWorkerMirroredStrategy, and ParameterServerStrategy. You should choose a distribution strategy based on the number of GPUs and machines available, the size of the model and dataset, and the communication overhead between devices."
      ],
      "metadata": {
        "id": "ApoK-8l0-Xfh"
      }
    }
  ]
}