{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Write the Python code to implement a single neuron.\n",
        "2. Write the Python code to implement ReLU.\n",
        "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
        "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
        "and functionality built into Python).\n",
        "5. What is the “hidden size” of a layer?\n",
        "6. What does the t method do in PyTorch?\n",
        "7. Why is matrix multiplication written in plain Python very slow?\n",
        "8. In matmul, why is ac==br?\n",
        "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
        "10. What is elementwise arithmetic?\n",
        "11. Write the PyTorch code to test whether every element of a is greater than the\n",
        "corresponding element of b.\n",
        "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
        "13. How does elementwise arithmetic help us speed up matmul?\n",
        "14. What are the broadcasting rules?\n",
        "15. What is expand_as? Show an example of how it can be used to match the results of\n",
        "broadcasting."
      ],
      "metadata": {
        "id": "uJs8uw9w0Cre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTION ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "7Lv3wstR0D7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Here's an example Python code to implement a single neuron using NumPy:\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Input vector of size n\n",
        "input_vector = np.array([1, 2, 3])\n",
        "\n",
        "# Weights vector of size n\n",
        "weights = np.array([0.5, 1.0, -0.5])\n",
        "\n",
        "# Bias term\n",
        "bias = 2.0\n",
        "\n",
        "# Calculate the dot product of input and weights, and add bias\n",
        "output = np.dot(input_vector, weights) + bias\n",
        "\n",
        "# Pass the output through a activation function (e.g. sigmoid, ReLU, etc.)\n",
        "# ...\n",
        "\n",
        "print(output)\n",
        "```\n",
        "\n",
        "2. Here's an example Python code to implement ReLU (Rectified Linear Unit) using NumPy:\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Example usage\n",
        "input_data = np.array([-2, -1, 0, 1, 2])\n",
        "output = relu(input_data)\n",
        "print(output)  # [0 0 0 1 2]\n",
        "```\n",
        "\n",
        "3. Here's an example Python code for a dense layer in terms of matrix multiplication using PyTorch:\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Input tensor of shape (batch_size, input_dim)\n",
        "input_tensor = torch.randn((64, 128))\n",
        "\n",
        "# Weight tensor of shape (input_dim, output_dim)\n",
        "weight_tensor = torch.randn((128, 64))\n",
        "\n",
        "# Bias tensor of shape (output_dim,)\n",
        "bias_tensor = torch.randn((64,))\n",
        "\n",
        "# Calculate the matrix multiplication between input and weight, and add bias\n",
        "output_tensor = torch.matmul(input_tensor, weight_tensor) + bias_tensor\n",
        "\n",
        "# Pass the output through an activation function (e.g. sigmoid, ReLU, etc.)\n",
        "# ...\n",
        "\n",
        "print(output_tensor.shape)  # (64, 64)\n",
        "```\n",
        "\n",
        "4. Here's an example Python code for a dense layer in plain Python (without using NumPy or PyTorch):\n",
        "```python\n",
        "def dense_layer(input_data, weights, bias):\n",
        "    output = [sum([input_data[j] * weights[j][i] for j in range(len(input_data))]) + bias[i] for i in range(len(bias))]\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "input_data = [1, 2, 3]\n",
        "weights = [[0.5, 1.0, -0.5], [1.0, -1.0, 0.5]]\n",
        "bias = [2.0, 1.0]\n",
        "output = dense_layer(input_data, weights, bias)\n",
        "print(output)  # [2.5, -1.5]\n",
        "```\n",
        "\n",
        "5. The \"hidden size\" of a layer refers to the number of neurons (i.e. units) in the layer. It represents the dimensionality of the output space of the layer.\n",
        "\n",
        "6. In PyTorch, the `t` method is used to transpose a tensor. It returns a new tensor with the dimensions reversed. For example:\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a tensor of shape (2, 3)\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Transpose the tensor\n",
        "x_t = x.t()\n",
        "\n",
        "print(x_t)\n",
        "# tensor([[1, 4],\n",
        "#         [2, 5],\n",
        "#         [3, 6]])\n",
        "```\n",
        "\n",
        "7. Matrix multiplication written in plain Python is very slow because it involves nested loops and a large number of individual operations, which can be inefficient in terms of memory access and caching.\n",
        "\n",
        "8. In matrix multiplication (`matmul`), `ac == br` is a requirement for the operation to be valid. Specifically,"
      ],
      "metadata": {
        "id": "AzRscchb0Pko"
      }
    }
  ]
}