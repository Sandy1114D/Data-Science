{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
        "randomly using He initialization?\n",
        "\n",
        "No, it is not recommended to initialize all the weights to the same value even if it is selected randomly using He initialization. Doing so would result in all neurons in a given layer computing the same function during training, which would lead to the neural network failing to learn any meaningful representation.\n",
        "\n",
        "2. Is it okay to initialize the bias terms to 0?\n",
        "\n",
        "Yes, it is generally okay to initialize the bias terms to 0, but other values may work better depending on the activation function being used. \n",
        "\n",
        "3. Name three advantages of the ELU activation function over ReLU.\n",
        "\n",
        "Three advantages of the Exponential Linear Unit (ELU) activation function over the Rectified Linear Unit (ReLU) activation function are:\n",
        "- ELU can produce negative outputs, which may help with avoiding the \"dying ReLU\" problem.\n",
        "- ELU is smooth everywhere, including around zero, which may help with faster convergence.\n",
        "- ELU can speed up learning in deeper networks compared to other activation functions.\n",
        "\n",
        "4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "- ELU: ELU is recommended to use in deep neural networks to avoid the \"dying ReLU\" problem, which can slow down or prevent convergence. ELU may also work well in some cases with image classification tasks and deep reinforcement learning.\n",
        "- Leaky ReLU: Leaky ReLU can be used as a replacement for ReLU when training deep neural networks and avoiding the \"dying ReLU\" problem. Leaky ReLU may also work well in some cases with image classification tasks and deep reinforcement learning.\n",
        "- ReLU: ReLU is a popular activation function that can work well in many cases for deep neural networks, especially in image classification tasks. However, it may lead to the \"dying ReLU\" problem when training very deep networks.\n",
        "- Tanh: Tanh can be used in cases where the input values to the neural network lie in the range [-1, 1]. It is often used in recurrent neural networks (RNNs) for natural language processing tasks.\n",
        "- Logistic: Logistic activation function is used for binary classification tasks, where the output of the neural network needs to be a probability value between 0 and 1.\n",
        "- Softmax: Softmax activation function is used in the output layer of a neural network to obtain the probability distribution over multiple classes in a multiclass classification task.\n",
        "\n",
        "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
        "\n",
        "If the momentum hyperparameter is set too close to 1, the optimizer will take a long time to converge or may not converge at all. In some cases, the optimizer may overshoot the minimum and oscillate back and forth.\n",
        "\n",
        "6. Name three ways you can produce a sparse model.\n",
        "\n",
        "Three ways to produce a sparse model are:\n",
        "- L1 regularization: This technique adds an L1 penalty term to the cost function, which encourages weights to be set to zero.\n",
        "- Dropout: This technique randomly drops out a certain percentage of neurons during training, forcing the network to rely on the remaining neurons and create a sparse representation.\n",
        "- Weight pruning: This technique removes the weights with the smallest magnitude from the neural network, resulting in a sparser model.\n",
        "\n",
        "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
        "\n",
        "Yes, dropout can slow down training since it requires randomly dropping out neurons during each"
      ],
      "metadata": {
        "id": "kkpwsVM67Zik"
      }
    }
  ]
}