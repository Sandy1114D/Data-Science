{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the Activation Functions in your own language\n",
        "a) sigmoid\n",
        "b) tanh\n",
        "c) ReLU\n",
        "d) ELU\n",
        "e) LeakyReLU\n",
        "f) swish\n",
        "\n",
        "2. What happens when you increase or decrease the optimizer learning rate?\n",
        "\n",
        "3. What happens when you increase the number of internal hidden neurons?\n",
        "\n",
        "4. What happens when you increase the size of batch computation?\n",
        "\n",
        "5. Why we adopt regularization to avoid overfitting?\n",
        "\n",
        "6. What are loss and cost functions in deep learning?\n",
        "\n",
        "7. What do ou mean by underfitting in neural networks?\n",
        "\n",
        "8. Why we use Dropout in Neural Networks?"
      ],
      "metadata": {
        "id": "pxL9yEvSEDbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTIONS ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "3wvPShrbEHBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Activation Functions:\n",
        "a) Sigmoid: Sigmoid is a commonly used activation function that maps any input to a value between 0 and 1. It's a smooth, S-shaped curve that is easy to work with and provides non-linear output. However, it has a problem of vanishing gradients for deep networks.\n",
        "b) Tanh: Tanh is another commonly used activation function that maps any input to a value between -1 and 1. Like sigmoid, it is also a smooth curve and provides non-linear output, but its output is zero-centered and has a steeper gradient than sigmoid.\n",
        "c) ReLU: ReLU stands for Rectified Linear Unit and is the most commonly used activation function today. It maps any input less than zero to zero and any input greater than or equal to zero to itself. ReLU is fast to compute and avoids the vanishing gradient problem. \n",
        "d) ELU: ELU stands for Exponential Linear Unit, which is similar to ReLU but with a smoother curve. It has negative values for negative input and exponential output for positive input.\n",
        "e) LeakyReLU: LeakyReLU is a modification of ReLU that addresses the \"dying ReLU\" problem by introducing a small slope for negative input. It's like ReLU for positive input and has a small negative slope for negative input.\n",
        "f) Swish: Swish is a newer activation function that has shown promising results in some cases. It's similar to sigmoid but with a non-linear input transformation that provides a wider range of input.\n",
        "\n",
        "2. Optimizer Learning Rate: The learning rate is a hyperparameter that determines how much the weights of the neural network should be adjusted with each iteration of the optimization algorithm. Increasing the learning rate can cause the network to converge faster, but if it's too high, the optimization algorithm can overshoot the optimal solution and diverge. Decreasing the learning rate can slow down the convergence, but it can help the network to converge to a more optimal solution.\n",
        "\n",
        "3. Number of Hidden Neurons: Increasing the number of hidden neurons can make the neural network more expressive and potentially able to learn more complex features from the data. However, it can also lead to overfitting if the network becomes too large for the given dataset. \n",
        "\n",
        "4. Batch Computation: Increasing the batch size can improve the convergence speed of the optimization algorithm and reduce the variance in the weight updates. However, it can also require more memory and computational resources, and if the batch size is too large, it can lead to suboptimal solutions.\n",
        "\n",
        "5. Regularization: Regularization techniques such as L1 and L2 regularization help to prevent overfitting by adding a penalty term to the cost function that discourages large weight values. This encourages the network to learn more general features that can be applied to new data, rather than just memorizing the training data.\n",
        "\n",
        "6. Loss and Cost Functions: The loss function measures the difference between the predicted output of the network and the actual output, while the cost function is the average loss over the entire training dataset. The choice of loss and cost functions depends on the problem being solved and the output type of the network.\n",
        "\n",
        "7. Underfitting: Underfitting occurs when a neural network is not able to learn the underlying patterns in the data, resulting in poor performance on both the training and test data. This can happen if the network is too simple or if it's not trained for long enough.\n",
        "\n",
        "8. Dropout: Dropout is a regularization technique that randomly drops out some of the neurons during each training iteration. This helps to prevent overfitting by encouraging the network to learn more robust and generalizable features."
      ],
      "metadata": {
        "id": "IptQlQ_7EOFq"
      }
    }
  ]
}