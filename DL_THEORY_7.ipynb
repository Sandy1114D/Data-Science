{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Can you think of a few applications for a sequence-to-sequence RNN? What about a\n",
        "sequence-to-vector RNN, and a vector-to-sequence RNN?\n",
        "2. How many dimensions must the inputs of an RNN layer have? What does each dimension\n",
        "represent? What about its outputs?\n",
        "3. If you want to build a deep sequence-to-sequence RNN, which RNN layers should\n",
        "have return_sequences=True? What about a sequence-to-vector RNN?\n",
        "4. Suppose you have a daily univariate time series, and you want to forecast the next seven\n",
        "days. Which RNN architecture should you use?\n",
        "5. What are the main difficulties when training RNNs? How can you handle them?\n",
        "6. Can you sketch the LSTM cell’s architecture?\n",
        "7. Why would you want to use 1D convolutional layers in an RNN?\n",
        "8. Which neural network architecture could you use to classify videos?\n",
        "9. Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets."
      ],
      "metadata": {
        "id": "nF8-f_CM8ek8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTIONS ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "Z0ZvVzq-8jZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A sequence-to-sequence RNN can be used for machine translation, speech recognition, and text summarization. A sequence-to-vector RNN can be used for sentiment analysis, document classification, and image captioning. A vector-to-sequence RNN can be used for generating text or music, and for speech synthesis.\n",
        "\n",
        "2. The inputs of an RNN layer must have three dimensions: batch size, sequence length, and feature dimension. The batch size represents the number of sequences processed at once, the sequence length is the length of each sequence, and the feature dimension is the number of features in each time step. The outputs of an RNN layer also have three dimensions.\n",
        "\n",
        "3. In a deep sequence-to-sequence RNN, all RNN layers except the last one should have return_sequences=True. In a sequence-to-vector RNN, only the last RNN layer should have return_sequences=False.\n",
        "\n",
        "4. For a daily univariate time series, you should use a stacked LSTM or GRU architecture, with a lookback window of seven days.\n",
        "\n",
        "5. The main difficulties when training RNNs are vanishing and exploding gradients, long-term dependencies, and overfitting. To handle these difficulties, you can use techniques such as gradient clipping, dropout, and regularization, and use architectures such as LSTMs and GRUs.\n",
        "\n",
        "6. The LSTM cell’s architecture consists of three gates: the input gate, the forget gate, and the output gate, which control the information flow into and out of the cell. The cell also has a memory cell that stores the previous cell state and a set of weights that control the gates’ behavior.\n",
        "\n",
        "7. 1D convolutional layers can be used in an RNN to capture local patterns and long-range dependencies more efficiently.\n",
        "\n",
        "8. A 3D convolutional neural network (CNN) architecture can be used to classify videos.\n",
        "\n",
        "9. Here is some example code to train a SketchRNN classification model using TensorFlow:\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the SketchRNN dataset\n",
        "ds, info = tfds.load('sketch_rnn/quickdraw', split='train', with_info=True)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(sample):\n",
        "    # Extract the sketch and label from the sample\n",
        "    sketch = sample['drawing']\n",
        "    label = sample['label']\n",
        "    # Convert the sketch to a sequence of (x, y, pen) tuples\n",
        "    sketch = tf.image.convert_image_dtype(sketch, tf.float32)\n",
        "    sketch = tf.transpose(sketch, perm=[1, 0, 2])\n",
        "    sketch = tf.reshape(sketch, [sketch.shape[0], -1])\n",
        "    sketch = tf.concat([sketch[:-1, :], sketch[1:, :]], axis=1)\n",
        "    # Return the preprocessed data\n",
        "    return sketch, label\n",
        "\n",
        "ds = ds.map(preprocess_data)\n",
        "ds = ds.batch(32).prefetch(1)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(None, 3)),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.Dense(345, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(ds, epochs=10)\n",
        "```\n",
        "\n",
        "This code loads the SketchRNN dataset from TensorFlow Datasets, preprocesses it, and trains a simple LSTM model to classify sketches into 345 categories."
      ],
      "metadata": {
        "id": "wKPkdI8L8oek"
      }
    }
  ]
}