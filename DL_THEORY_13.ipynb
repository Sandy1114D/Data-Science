{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
        "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
        "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
        "classifier?\n",
        "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
        "3. Name three popular activation functions. Can you draw them?\n",
        "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
        "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
        "artificial neurons. All artificial neurons use the ReLU activation function.\n",
        " What is the shape of the input matrix X?\n",
        " What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
        "bias vector bh?\n",
        " What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
        " What is the shape of the network’s output matrix Y?\n",
        " Write the equation that computes the network’s output matrix Y as a function\n",
        "of X, Wh, bh, Wo and bo.\n",
        "\n",
        "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
        "or ham? What activation function should you use in the output layer? If instead you want to\n",
        "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
        "function?\n",
        "6. What is backpropagation and how does it work? What is the difference between\n",
        "backpropagation and reverse-mode autodiff?\n",
        "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
        "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
        "adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
        "an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
      ],
      "metadata": {
        "id": "sfA2WHEY6s-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF FOLLOWING QUESTIONS ARE AS FOLLLOW**"
      ],
      "metadata": {
        "id": "dr8W_4jY6w_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. It is generally preferable to use a Logistic Regression classifier over a Perceptron because Logistic Regression outputs class probabilities and can also learn complex decision boundaries by using a non-linear transformation of the inputs. On the other hand, the Perceptron algorithm is limited to linearly separable classes and cannot output probabilities. To make a Perceptron equivalent to a Logistic Regression classifier, we can replace its step function with the logistic function (i.e., sigmoid function) to obtain class probabilities.\n",
        "\n",
        "2. The logistic activation function was a key ingredient in training the first MLPs because it is differentiable and can be used to optimize the weights and biases of the network using gradient descent. The logistic function has a smooth derivative that is easily computable, which makes it well-suited for gradient-based optimization algorithms.\n",
        "\n",
        "3. Three popular activation functions are:\n",
        "- ReLU (Rectified Linear Unit): f(x) = max(0, x)\n",
        "- Sigmoid: f(x) = 1 / (1 + exp(-x))\n",
        "- Tanh (Hyperbolic Tangent): f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "\n",
        "4. \n",
        "- Shape of input matrix X: (batch_size, 10)\n",
        "- Shape of hidden layer's weight vector Wh: (10, 50)\n",
        "- Shape of hidden layer's bias vector bh: (50,)\n",
        "- Shape of output layer's weight vector Wo: (50, 3)\n",
        "- Shape of output layer's bias vector bo: (3,)\n",
        "- Shape of network's output matrix Y: (batch_size, 3)\n",
        "- Equation that computes the network's output matrix Y as a function of X, Wh, bh, Wo, and bo: Y = relu(X.dot(Wh) + bh).dot(Wo) + bo\n",
        "\n",
        "5. If we want to classify email into spam or ham, we only need 1 neuron in the output layer with the sigmoid activation function. If we want to tackle MNIST, we need 10 neurons in the output layer with the softmax activation function.\n",
        "\n",
        "6. Backpropagation is a method for computing the gradients of the loss function with respect to the weights and biases of a neural network using the chain rule of calculus. It works by first forward-propagating the input through the network to compute the output, then computing the error between the predicted output and the true output, and finally backpropagating the error through the network to compute the gradients. The difference between backpropagation and reverse-mode autodiff is that backpropagation is a specific implementation of the chain rule that is tailored to neural networks, whereas reverse-mode autodiff is a more general method for computing gradients of arbitrary functions.\n",
        "\n",
        "7. Some hyperparameters we can tweak in an MLP are:\n",
        "- Number of hidden layers and number of neurons per layer\n",
        "- Learning rate\n",
        "- Activation function\n",
        "- Regularization (e.g., L1 or L2 regularization)\n",
        "- Dropout rate\n",
        "- Batch size\n",
        "- Number of epochs\n",
        "If the MLP overfits the training data, we can tweak these hyperparameters by:\n",
        "- Reducing the number of neurons per layer or the number of layers\n",
        "- Increasing the regularization strength\n",
        "- Increasing the dropout rate\n",
        "- Decreasing the learning rate\n",
        "- Increasing the batch size\n",
        "\n",
        "8. Training a deep MLP on the MNIST dataset and getting over 98% precision can be done using various methods such as using convolutional layers, data augmentation, or tuning hyperparameters. To add bells and whistles, we can save checkpoints using TensorFlow's ModelCheckpoint callback, restore the last checkpoint in case of an interruption using the same callback, add summaries using TensorFlow's TensorBoard, and plot learning curves using matplotlib or seaborn."
      ],
      "metadata": {
        "id": "JQMuJSaI63MR"
      }
    }
  ]
}