{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
        "randomly using He initialization?\n",
        "2. Is it OK to initialize the bias terms to 0?\n",
        "3. Name three advantages of the SELU activation function over ReLU.\n",
        "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
        "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
        "when using an SGD optimizer?\n",
        "6. Name three ways you can produce a sparse model.\n",
        "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
        "new instances)? What about MC Dropout?\n",
        "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
        "point of this exercise). Use He initialization and the ELU activation function.\n",
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
        "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
        "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
        "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
        "Remember to search for the right learning rate each time you change the model’s\n",
        "architecture or hyperparameters.\n",
        "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
        "converging faster than before? Does it produce a better model? How does it affect\n",
        "training speed?\n",
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
        "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
        "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
        "layers, etc.).\n",
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
        "see if you can achieve better accuracy using MC Dropout."
      ],
      "metadata": {
        "id": "C16WlHpz5yu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTIONS ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "Xfku-iub52OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initializing all weights to the same value using He initialization is not recommended. While He initialization ensures that the variance of the outputs of each layer is roughly the same as the variance of its inputs, initializing all weights to the same value would result in each neuron in a layer computing the same function, making it no different than having a single neuron in that layer. \n",
        "2. Initializing bias terms to 0 is a common practice and generally works well, especially when using an activation function like ReLU or its variants. \n",
        "3. Three advantages of the SELU activation function over ReLU are:\n",
        "   - SELU can self-normalize, which means that it maintains a stable output mean and variance throughout training, making it less sensitive to weight initialization and leading to faster convergence.\n",
        "   - SELU can be used with dropout without requiring any additional scaling or shifting, unlike ReLU and its variants, which may need to be adjusted to maintain the same level of regularization.\n",
        "   - SELU can produce negative outputs, which can be useful in some scenarios where negative values are meaningful, unlike ReLU, which always produces non-negative outputs.\n",
        "4. In general, it is recommended to use ReLU or one of its variants (e.g., leaky ReLU) as the default activation function for hidden layers, while using sigmoid or tanh for the output layer depending on the nature of the problem. Softmax is typically used for multi-class classification problems. SELU is recommended for deep networks that require self-normalization and can be used in place of ReLU. \n",
        "5. Setting the momentum hyperparameter too close to 1 can cause the optimizer to overshoot the minimum of the cost function, making it difficult for the optimizer to converge to the minimum. This can lead to slow convergence or even divergence of the optimization process. \n",
        "6. Three ways to produce a sparse model are:\n",
        "   - L1 regularization: This encourages the model to learn sparse weights by adding a penalty term to the cost function that is proportional to the sum of absolute values of the weights.\n",
        "   - Dropout: This randomly drops out a certain percentage of neurons during training, forcing the remaining neurons to learn more robust features and preventing overfitting.\n",
        "   - Batch normalization: This can help produce a sparse model by reducing the covariance shift between layers, which in turn reduces the need for each layer to learn redundant features.\n",
        "7. Dropout can slow down training since it requires more iterations to converge, but it can also prevent overfitting and lead to better generalization performance. It does not slow down inference since all neurons are active during inference. MC Dropout can be slower than dropout during inference since it involves running the model multiple times with different dropout masks, but it can lead to more accurate predictions by estimating the model's uncertainty. \n",
        "8. This is a practical exercise and requires hands-on coding, so we cannot provide a direct answer here. However, some general tips for training a deep neural network on CIFAR10 are:\n",
        "   - Use a suitable optimizer, such as Adam or Nadam, with a reasonable learning rate.\n",
        "   - Use a suitable activation function for the hidden layers, such as ReLU or its variants.\n",
        "   - Use He initialization to ensure a proper weight initialization.\n",
        "   - Use early stopping to avoid overfitting and select the optimal number of epochs.\n",
        "   - Add Batch Normalization to accelerate convergence and improve performance.\n",
        "   - Use SELU to enable self-normalization and improve performance even further.\n",
        "   - Use alpha dropout to regularize the model and MC Dropout to estimate the model's uncertainty."
      ],
      "metadata": {
        "id": "U-XE5dhw57Rn"
      }
    }
  ]
}