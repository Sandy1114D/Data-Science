{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
        "2. Why do people use Encoderâ€“Decoder RNNs rather than plain sequence-to-sequence RNNs\n",
        "for automatic translation?\n",
        "3. How can you deal with variable-length input sequences? What about variable-length output\n",
        "sequences?\n",
        "4. What is beam search and why would you use it? What tool can you use to implement it?\n",
        "5. What is an attention mechanism? How does it help?\n",
        "6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
        "7. When would you need to use sampled softmax?"
      ],
      "metadata": {
        "id": "pECpZJZn861y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTIONS ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "ImP9RRf19CIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The main advantage of a stateful RNN is that it can remember information from previous batches, which is important when dealing with sequences that are too long to fit into memory all at once. This allows the RNN to process very long sequences, or to perform online learning on streams of data. The main disadvantage is that it requires careful handling of the batch size, and can be sensitive to changes in the input sequence length. In contrast, a stateless RNN processes each batch independently, which can make it simpler to use and more robust to changes in the input sequence length, but it cannot remember information from previous batches.\n",
        "\n",
        "2. Encoder-Decoder RNNs are better suited for automatic translation than plain sequence-to-sequence RNNs because they can handle input and output sequences of different lengths. The encoder processes the input sequence and produces a fixed-length context vector, which is then used by the decoder to generate the output sequence. This allows the model to learn a representation of the input sequence that can be used to generate a variable-length output sequence.\n",
        "\n",
        "3. To deal with variable-length input sequences, one common approach is to use padding to ensure that all sequences have the same length. The padded elements are typically ignored by the RNN. To deal with variable-length output sequences, one common approach is to use an end-of-sequence token to indicate the end of the sequence, and then stop generating output once the token is produced.\n",
        "\n",
        "4. Beam search is a search algorithm that can be used to generate the most likely sequence of outputs in sequence-to-sequence models. It works by maintaining a set of the most promising partial sequences at each time step, and expanding them based on the probabilities of the next output token. This can lead to better results than simply choosing the most likely token at each time step, as it allows the model to explore alternative paths. Beam search can be implemented using the tf.contrib.seq2seq.BeamSearchDecoder tool in TensorFlow.\n",
        "\n",
        "5. An attention mechanism is a technique used in sequence-to-sequence models that allows the decoder to selectively focus on different parts of the input sequence when generating the output sequence. It works by computing a set of attention weights for each input element, which are then used to compute a weighted sum of the input sequence. This allows the model to assign more weight to relevant parts of the input sequence, and can improve the accuracy of the model.\n",
        "\n",
        "6. The most important layer in the Transformer architecture is the self-attention layer. Its purpose is to allow the model to selectively attend to different parts of the input sequence when generating the output sequence. It works by computing a set of attention weights for each input element, which are then used to compute a weighted sum of the input sequence. The self-attention layer is used in both the encoder and decoder of the Transformer architecture.\n",
        "\n",
        "7. Sampled softmax is used when the output vocabulary is very large, as it can be computationally expensive to compute the softmax over all possible output tokens. Sampled softmax works by randomly selecting a subset of the output tokens and computing the softmax over this subset. This can be faster than computing the full softmax, but can also introduce noise into the output predictions."
      ],
      "metadata": {
        "id": "siMZqXiu9GNN"
      }
    }
  ]
}