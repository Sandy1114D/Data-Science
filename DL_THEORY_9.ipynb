{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are the main tasks that autoencoders are used for?\n",
        "2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but\n",
        "only a few thousand labeled instances. How can autoencoders help? How would you\n",
        "proceed?\n",
        "3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder?\n",
        "How can you evaluate the performance of an autoencoder?\n",
        "4. What are undercomplete and overcomplete autoencoders? What is the main risk of an\n",
        "excessively undercomplete autoencoder? What about the main risk of an overcomplete\n",
        "autoencoder?\n",
        "5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
        "6. What is a generative model? Can you name a type of generative autoencoder?\n",
        "7. What is a GAN? Can you name a few tasks where GANs can shine?\n",
        "8. What are the main difficulties when training GANs?"
      ],
      "metadata": {
        "id": "0a8wXxqe9sAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTION ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "kn77J7Sp9wQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Autoencoders are used for tasks such as data compression, feature extraction, anomaly detection, denoising, and image generation.\n",
        "2. Autoencoders can help in such cases by learning a compressed representation of the unlabeled data that can be used to improve the classifier's performance. The approach is to pretrain an autoencoder on the unlabeled data and use the learned representation as input to the classifier, which is then fine-tuned on the labeled data.\n",
        "3. No, perfect reconstruction does not necessarily mean good performance. One way to evaluate the performance of an autoencoder is to measure how well it performs on a specific task that the autoencoder is designed for, such as classification or anomaly detection.\n",
        "4. An undercomplete autoencoder has a smaller bottleneck layer than the input layer, while an overcomplete autoencoder has a larger bottleneck layer. The main risk of an excessively undercomplete autoencoder is that it may not be able to reconstruct the input data accurately. The main risk of an overcomplete autoencoder is that it may simply learn to copy the input data to the output, without learning any useful features.\n",
        "5. Tying weights in a stacked autoencoder means sharing the weights between the encoder and decoder layers. The point of doing so is to reduce the number of parameters and prevent overfitting.\n",
        "6. A generative model is a type of model that can generate new data samples that are similar to the training data. A type of generative autoencoder is a variational autoencoder (VAE).\n",
        "7. A GAN (Generative Adversarial Network) is a type of neural network that can generate new data samples by learning to mimic the distribution of the training data. GANs can shine in tasks such as image generation, style transfer, and data augmentation.\n",
        "8. The main difficulties when training GANs include mode collapse, where the generator produces only a limited set of output samples, and instability, where the generator and discriminator fail to learn effectively and oscillate between states. Other difficulties include vanishing gradients, lack of convergence, and sensitivity to hyperparameters."
      ],
      "metadata": {
        "id": "BUF6i9UJ92om"
      }
    }
  ]
}