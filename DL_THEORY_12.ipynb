{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
        "2. How can we use indexing to do the same operation as unsqueeze?\n",
        "3. How do we show the actual contents of the memory used for a tensor?\n",
        "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
        "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
        "code in a notebook.)\n",
        "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
        "6. Implement matmul using Einstein summation.\n",
        "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
        "8. What are the three rules of Einstein summation notation? Why?\n",
        "9. What are the forward pass and backward pass of a neural network?\n",
        "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
        "forward pass?\n",
        "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
        "12. How can weight initialization help avoid this problem?"
      ],
      "metadata": {
        "id": "HsEDl1vq5a63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANS OF ABOVE QUESTION ARE AS FOLLOW**"
      ],
      "metadata": {
        "id": "Yn1vLCWe5hn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The unsqueeze function can be used to add a singleton dimension to a tensor, which can help with broadcasting problems.\n",
        "2. We can use indexing with a None keyword to achieve the same result as unsqueeze. For example, given a tensor x with shape (3,), we can add a singleton dimension by using x[:, None] or x.reshape(3, 1).\n",
        "3. We can use the print function to show the actual contents of the memory used for a tensor.\n",
        "4. When adding a vector of size 3 to a matrix of size 3×3, the elements of the vector are added to each row of the matrix.\n",
        "5. Broadcasting and expand_as do not result in increased memory use, as they do not create new tensors but instead create views of existing tensors.\n",
        "6. Here is an example implementation of matmul using Einstein summation in PyTorch: torch.einsum('ij,jk->ik', x, y).\n",
        "7. A repeated index letter on the lefthand side of einsum represents a summation over that index. For example, in the expression 'ij,jk->ik', the repeated index letter 'j' represents a summation over that index.\n",
        "8. The three rules of Einstein summation notation are:\n",
        "- If an index appears exactly twice in an expression, it represents a summation over that index.\n",
        "- If an index appears exactly once in an expression, it represents a singleton dimension.\n",
        "- If an index does not appear in an expression, it represents a free index.\n",
        "These rules help simplify and express complex tensor operations in a concise and intuitive way.\n",
        "9. The forward pass of a neural network involves passing input data through the layers of the network to produce an output prediction. The backward pass involves computing the gradients of the loss function with respect to the parameters of the network, using the chain rule to backpropagate the error through the layers.\n",
        "10. Storing some of the activations calculated for intermediate layers in the forward pass can help speed up the backward pass during training, as the gradients can be computed more efficiently using the stored activations rather than recalculating them.\n",
        "11. The downside of having activations with a standard deviation too far away from 1 is that it can lead to either vanishing or exploding gradients during backpropagation, which can make training difficult or impossible.\n",
        "12. Weight initialization can help avoid the problem of vanishing or exploding gradients by setting the initial weights to values that are appropriate for the activation functions used in the network. For example, initializing the weights using the He or Xavier initialization methods can help ensure that the standard deviation of the activations remains close to 1, which can improve the stability and convergence of the network during training."
      ],
      "metadata": {
        "id": "M28Kfj9r5qt9"
      }
    }
  ]
}